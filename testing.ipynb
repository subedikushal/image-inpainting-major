{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from celeb import CelebDatasetFast\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.PILToTensor(), transforms.Lambda(lambda x: x/255), transforms.Resize([178,178], antialias=True)])\n",
    "\n",
    "dataset_size = 1000\n",
    "batch_size = 5\n",
    "train_dataset = CelebDatasetFast(\n",
    "    split='train', transform=transform,total=dataset_size)\n",
    "\n",
    "test_dataset = CelebDatasetFast(\n",
    "    split='test', transform=transform,total=dataset_size)\n",
    "\n",
    "val_dataset = CelebDatasetFast(\n",
    "    split='val', transform=transform,total=dataset_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, False)\n",
    "val_loader = DataLoader(val_dataset, batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (178) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[1;32m      2\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[0;32m----> 3\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m mask,inp\u001b[38;5;241m=\u001b[39m samples[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m samples[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/project/major/celeb.py:119\u001b[0m, in \u001b[0;36mCelebDatasetFast.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_name)\n\u001b[1;32m    118\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m,mask,img\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (178) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_loader))\n",
    "examples = iter(train_loader)\n",
    "samples = next(examples)\n",
    "mask,inp= samples[0]\n",
    "target = samples[1]\n",
    "print(inp.shape)\n",
    "print(inp[0])\n",
    "print(target.shape)\n",
    "\n",
    "for k in range(0, 6, 2):\n",
    "    i = inp[k].permute((1, 2, 0))\n",
    "    plt.subplot(6, 2, k+1)\n",
    "    plt.axis(\"off\")\n",
    "    if k == 0:\n",
    "        plt.title(\"Input\")\n",
    "    plt.imshow(i)\n",
    "    o = target[k].permute((1, 2, 0))\n",
    "    plt.subplot(6, 2, k+2)\n",
    "    plt.axis(\"off\")\n",
    "    if k == 0:\n",
    "        plt.title(\"Target\")\n",
    "    plt.imshow(o)\n",
    "\n",
    "plt.subplots_adjust(left=0.05,\n",
    "                    bottom=0.05,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0,\n",
    "                    hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "\n",
    "        # down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                # output = s * (n-1) + k- 2*p\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature,kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2,feature))\n",
    "        \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:], antialias=True)\n",
    "            concat_skip = torch.cat((skip_connection, x), dim = 1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        \n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNET(3,3)\n",
    "#summary(model,(3,178,178));\n",
    "model.load_state_dict(torch.load(\"./models/20_loss.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def tensorToPIL(t):\n",
    "    return torchvision.transforms.functional.to_pil_image(t, \"RGB\")\n",
    "\n",
    "\n",
    "def save_tensor_as_image(t,name):\n",
    "    img = tensorToPIL(t) \n",
    "    # img.show()\n",
    "    img.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    h,w = 178,178\n",
    "    model = model.to('cpu')\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size, True)\n",
    "    examples = iter(test_loader)\n",
    "\n",
    "    samples, targets = next(examples)\n",
    "\n",
    "    inputs = samples[1]\n",
    "    masks = samples[0]\n",
    "\n",
    "    outputs = model(samples[1])\n",
    "\n",
    "\n",
    "    # print(outputs.shape)\n",
    "    # save_tensor_as_image(outputs[1], \"ouput.png\")\n",
    "    rows = 3\n",
    "    cols = 3\n",
    "    for i in range(1,rows*cols,cols):\n",
    "        input = inputs[i%batch_size].reshape(3, h, h).permute(1,2,0)\n",
    "        # input = input.cpu().numpy()\n",
    "\n",
    "\n",
    "        output = outputs[i%batch_size].reshape(3, h, h).permute(1,2,0)\n",
    "        output = torch.clamp(output, 0, 1)\n",
    "        # output = output.cpu().numpy()\n",
    "\n",
    "        target = targets[i%batch_size].reshape(3, h, h).permute(1,2,0)\n",
    "        # target = target.cpu().numpy()\n",
    "\n",
    "        # print(output.shape)\n",
    "        # input\n",
    "        plt.subplot(rows,cols,i)\n",
    "        plt.axis(\"off\")\n",
    "        if i == 1:\n",
    "            plt.title(\"Input\")\n",
    "        plt.imshow(input)\n",
    "        #output\n",
    "        plt.subplot(rows,cols,i+1)\n",
    "        plt.axis(\"off\")\n",
    "        if i == 1:\n",
    "            plt.title(\"Output\")\n",
    "        plt.imshow(output)\n",
    "        # ground truth\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(rows,cols,i+2)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        if i == 1:\n",
    "            plt.title(\"Ground Truth\")\n",
    "        plt.imshow(target)\n",
    "    plt.subplots_adjust(left=0.05,\n",
    "                        bottom=0.05,\n",
    "                        right=0.9,\n",
    "                        top=0.9,\n",
    "                        wspace=0.1,\n",
    "                        hspace=0.1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celeb import gen_line_mask\n",
    "mask = torch.from_numpy(gen_line_mask((178, 178, 3), (8, 18))).permute((2,0,1))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_image(img):\n",
    "    h,w = 178, 178\n",
    "\n",
    "    i = Image.open(img)\n",
    "    transform = transforms.Compose([ \n",
    "        transforms.PILToTensor(),\n",
    "        transforms.Resize((h,w), antialias=False),\n",
    "        transforms.Lambda(lambda x: x/255),\n",
    "    ]) \n",
    "\n",
    "    img_tensor = transform(i)\n",
    "\n",
    "    return torch.unsqueeze(img_tensor, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(3,3)\n",
    "path_to_model = \"./models/20_loss.pth\"\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 178, 178])\n"
     ]
    }
   ],
   "source": [
    "input_img = prepare_single_image(\"./trainthick/traininput/6.png\")\n",
    "print(input_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.clamp(output[0],0.,1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:9331): EOG-CRITICAL **: 09:57:45.685: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:9331): GLib-GIO-CRITICAL **: 09:57:45.685: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_tensor_as_image(input_img[0], \"input.png\")\n",
    "save_tensor_as_image(image_tensor, \"ouput.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def gen_test_dataset(path_to_images, save_to_dir):\n",
    "\n",
    "    h,w = 178, 178\n",
    "\n",
    "    for image in os.listdir(path_to_images):\n",
    "        mask = torch.from_numpy(gen_line_mask((178, 178, 3), (7, 12))).permute((2,0,1))/255\n",
    "        path = os.path.join(path_to_images, image)\n",
    "        if os.path.isfile(path):\n",
    "            i = Image.open(path)\n",
    "            transform = transforms.Compose([ \n",
    "                transforms.PILToTensor(),\n",
    "                transforms.Resize((h,w), antialias=False),\n",
    "                transforms.Lambda(lambda x: x/255),\n",
    "            ]) \n",
    "            img_tensor = transform(i)\n",
    "            masked_img_tensor = torch.maximum(mask,img_tensor)\n",
    "            save_tensor_as_image(masked_img_tensor, f\"./{save_to_dir}/{image}\")\n",
    "\n",
    "\n",
    "gen_test_dataset(\"./test_images\", \"./test_dataset\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n",
      "Image downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "for i in range(10):\n",
    "    response = requests.get(\"https://thispersondoesnotexist.com/\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Open a file in binary write mode and write the content of the response to the file\n",
    "        with open(f\"./test_images/image_{i}.png\", 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(\"Image downloaded successfully!\")\n",
    "    else:\n",
    "        print(\"Failed to download image:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:9331): EOG-CRITICAL **: 21:46:12.691: eog_window_ui_settings_changed_cb: assertion 'G_IS_ACTION (user_data)' failed\n",
      "\n",
      "(eog:9331): EOG-CRITICAL **: 21:46:12.691: eog_window_ui_settings_changed_cb: assertion 'G_IS_ACTION (user_data)' failed\n",
      "\n",
      "(eog:9331): EOG-CRITICAL **: 21:46:12.691: eog_window_ui_settings_changed_cb: assertion 'G_IS_ACTION (user_data)' failed\n",
      "\n",
      "(eog:9331): EOG-CRITICAL **: 21:46:12.691: eog_window_ui_settings_changed_cb: assertion 'G_IS_ACTION (user_data)' failed\n"
     ]
    }
   ],
   "source": [
    "mask = torch.from_numpy(gen_line_mask((178, 178, 3), (7, 12))).permute((2,0,1))/255\n",
    "image = Image.open(\"./6.png\")\n",
    "transform = transforms.Compose([ \n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Resize((178,178), antialias=False),\n",
    "    transforms.Lambda(lambda x: x/255),\n",
    "]) \n",
    "img_tensor = transform(image)\n",
    "save_tensor_as_image(mask, \"mask.png\")\n",
    "save_tensor_as_image(torch.maximum(mask, image_tensor), \"masked_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
