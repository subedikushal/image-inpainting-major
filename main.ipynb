{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from celeb import CelebDatasetFast\n",
    "from torchsummary import summary\n",
    "from models import UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 \n",
    "dataset_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.PILToTensor(), transforms.Lambda(lambda x: x/255), transforms.Resize([178,178], antialias=True)])\n",
    "\n",
    "\n",
    "train_dataset = CelebDatasetFast(\n",
    "    split='train', transform=transform,total=dataset_size)\n",
    "\n",
    "test_dataset = CelebDatasetFast(\n",
    "    split='test', transform=transform,total=dataset_size)\n",
    "\n",
    "val_dataset = CelebDatasetFast(\n",
    "    split='val', transform=transform,total=dataset_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, False)\n",
    "val_loader = DataLoader(val_dataset, batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(train_loader))\n",
    "examples = iter(train_loader)\n",
    "samples = next(examples)\n",
    "inp= samples[0]\n",
    "mask = samples[1]\n",
    "target = samples[2]\n",
    "# print(inp.shape)\n",
    "# print(inp[0])\n",
    "# print(target.shape)\n",
    "\n",
    "for k in range(0, 6, 2):\n",
    "    i = inp[k].permute((1, 2, 0))\n",
    "    plt.subplot(6, 2, k+1)\n",
    "    if k == 0:\n",
    "        plt.title(\"Input\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(i)\n",
    "    o = target[k].permute((1, 2, 0))\n",
    "    plt.subplot(6, 2, k+2)\n",
    "    if k == 0:\n",
    "        plt.title(\"Output\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(o)\n",
    "\n",
    "plt.subplots_adjust(left=0.05,\n",
    "                    bottom=0.05,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.1,\n",
    "                    hspace=0.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"train.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_epochs = 10 \n",
    "learning_rate = 1e-3 #better is 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(3,3).to(device)\n",
    "criterion =nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "train_losses= {}\n",
    "val_losses = {}\n",
    "min_loss = float(\"inf\")\n",
    "for epoch in tqdm(range(7, num_epochs+1), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for input_images,masks, target_images in tqdm(train_loader,desc=f\"Training batches\", unit=\"batch\", leave=False):\n",
    "        input_images = input_images.to(device)\n",
    "        masks = input_images.to(device)\n",
    "        target_images = target_images.to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs = model(input_images)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        # train_loss = criterion(input_images, masks, outputs, target_images)\n",
    "        train_loss = criterion(outputs, target_images) \n",
    "        # print(train_loss)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        training_loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    training_loss = training_loss / len(train_loader)\n",
    "    train_losses[epoch] = training_loss\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        validation_loss = 0\n",
    "        for input_images,masks,target_images in tqdm(val_loader,desc=f\"Validation batches\", unit=\"batch\", leave=False):\n",
    "            input_images = input_images.to(device)\n",
    "            masks = input_images.to(device)\n",
    "            target_images = target_images.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = model(input_images)\n",
    "            \n",
    "            # compute training reconstruction loss\n",
    "            # val_loss = criterion(input_images, masks, outputs, target_images)\n",
    "            val_loss = criterion(outputs, target_images)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # add the mini-batch training loss to epoch loss\n",
    "            validation_loss += val_loss.item()\n",
    "        validation_loss = validation_loss / len(val_loader)\n",
    "        val_losses[epoch] = validation_loss\n",
    "\n",
    "    \n",
    "    # display the epoch training loss\n",
    "    if training_loss < min_loss:\n",
    "        min_loss = training_loss\n",
    "        torch.save(model.state_dict(), f'{epoch}_epoch.pth')\n",
    "\n",
    "    print(\"Epoch : {}/{}, Training Loss = {:.6f}, Validation Loss = {:.6f}\".format(epoch, num_epochs, training_loss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format the current date as a string in the \"YYYY-MM-DD\" format\n",
    "formatted_date = current_datetime.strftime(\"%d-%m-%Y(%H-%M)\")\n",
    "\n",
    "import json\n",
    "with open(f'trainlosses_{formatted_date}.json', 'w') as fp:\n",
    "    json.dump(train_losses, fp)\n",
    "\n",
    "with open(f'vallosses_{formatted_date}.json', 'w') as fp:\n",
    "    json.dump(val_losses, fp)\n",
    "\n",
    "torch.save(model.state_dict(), f'{formatted_date}.pth')\n",
    "print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def tensorToPIL(t):\n",
    "    return torchvision.transforms.functional.to_pil_image(t, \"RGB\")\n",
    "\n",
    "\n",
    "def save_tensor_as_image(t,name):\n",
    "    img = tensorToPIL(t) \n",
    "    img.show()\n",
    "    img.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    h,w = 178,178\n",
    "    model = model.to('cpu')\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size, True)\n",
    "    examples = iter(test_loader)\n",
    "\n",
    "    inputs,masks, targets = next(examples)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "\n",
    "    # print(outputs.shape)\n",
    "    # save_tensor_as_image(outputs[1], \"ouput.png\")\n",
    "    rows = 3\n",
    "    cols = 3\n",
    "    for i in range(1,rows*cols,cols):\n",
    "        input = inputs[i%batch_size].reshape(3, h, h).permute(1,2,0)\n",
    "        # input = input.cpu().numpy()\n",
    "\n",
    "        output = outputs[i%batch_size].reshape(3, h, h).permute(1,2,0)\n",
    "        # output = output.cpu().numpy()\n",
    "\n",
    "        target = targets[i%batch_size].reshape(3, h, h).permute(1,2,0)\n",
    "        # target = target.cpu().numpy()\n",
    "\n",
    "        # print(output.shape)\n",
    "        # input\n",
    "        plt.subplot(rows,cols,i)\n",
    "        if i == 1:\n",
    "            plt.title(\"Input\")\n",
    "        plt.imshow(input)\n",
    "        #output\n",
    "        plt.subplot(rows,cols,i+1)\n",
    "        if i == 1:\n",
    "            plt.title(\"Output\")\n",
    "        plt.imshow(output)\n",
    "        # ground truth\n",
    "        plt.subplot(rows,cols,i+2)\n",
    "\n",
    "        if i == 1:\n",
    "            plt.title(\"Ground Truth\")\n",
    "        plt.imshow(target)\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                        bottom=0.05,\n",
    "                        right=0.9,\n",
    "                        top=0.9,\n",
    "                        wspace=0.4,\n",
    "                        hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
